## Appendix A: Methodological Explanations

### When considering black developers in USA, why would language play a role? Why we think that differences in race and ethnicity leads to differences in culture/thinking about the process/product?
We have not restricted our datasets to developers in USA in our study. Our data is formed by developers from all over the world. We have used the U.S. Census categorization for identifying the most likely estimate race and ethnicity of GitHub developers in our dataset as the tools we used for estimating the race and ethnicity of GitHub developers are based on the he U.S. Census categorization. 

In our study, developers' language barriers would play a role as not all black developers are English native speakers. Moreover, if we wanted to narrow down our dataset to only USA developers, we couldn't be sure whether these developers are actually native English speakers or not. Developers can add the location/country where they live in their GitHub profiles, but that location/country can be temporary and might not capture their nationality. So at the end, our dataset would still contain the language barrier.

Our believe is that differences in race and ethnicity leads to differences in thinking about the processes and products because developers from different races and ethnicities have different backgrounds and experiences, and they can think on the product from different perspectives. To provide more context about this, we would like to add a few examples of how these differences can shape software products: (1) The best algorithms struggle to recognize Black faces equally. We believe that if more black developers would have been part of the development/training of these algorithms, they might have had a better accuracy in recognizing black faces, or (2) GitHub user interface has not been ``internationalized'', all users independently of their language interact with an English user interface. If cultural diverse developers would have been involved during the development or tests of the GitHub user interface, maybe GitHub would have been translate to different languages from the beginning.

### The entire race and ethnicity construct used in the paper is based on the US Census data, however GitHub is International and in other countries the same type of systemic biases that exist against Blacks/Latinos as in the USA may not be present. Why did we not scope the research/data to US or US+Canada locations?
Although we have used the U.S. Census categorization to estimate the race and ethnicity of GitHub developers, we believe that these categorizations encompass the main racial and ethnic groups worldwide. Indeed, similar categorization can be found in the European Census: Afro/Black-Europeans, White, Asian, or Mixed https://ec.europa.eu/info/sites/default/files/data_collection_in_the_field_of_ethnicity.pdf.

We agree that in other countries the same type of systemic biases that exist against Blacks/Latinos as in the USA may not be present. However, GitHub is a worldwide platform in which OSS projects are open to contributions from all parts of the world. OSS projects are not restricted to just national contributions. We believe that if a USA White developer have unconscious or conscious bias towards a Black developer, it does not matter whether the Black developer lives in U.S., Europe, or Africa. Similar on the other way around. In addition, if we select only USA developers, we cannot be sure whether these developers were actually born, and/or raised in USA using the geotags provided in GitHub. Developers can add the location/country where they live in their GitHub profiles, but that location/country can be temporary and might not capture their nationality. So, restricting the dataset only to USA developers might include additional threats to the validity of the results.

### Why did we have different confidences in race for the different subgroups (e.g., 60\% for API, 70\% for Hispanics)?
Our previous study https://arxiv.org/pdf/2104.06143.pdf used a threshold of 0.8 for all races and ethnicities. We selected 0.8 as a threshold based on a sensitivity analysis. For that sensitivity analysis we built three models by setting three different thresholds (0.7, 0.8, and 0.9) for all races and ethnicities. From the output of the sensitivity analysis, we saw that the results from the three models were stable. In addition, we analyzed the number of false positives and negatives in each one of the models. When choosing the threshold 0.7 for all races and ethnicities, the number of false negatives was 0 but, the number of false positive was the highest. Contrary, when choosing the threshold 0.9 for all races and ethnicities, the number of false positives was the lowest but, the number of false negative was the highest. Therefore, we chose the threshold 0.8 because the number of false positives and false negatives were balanced when comparing with 0.7 and 0.9 and.

For this study, we wanted to target the accuracy of NamePrism for different races and ethnicities, so we did a qualitative analysis. In this qualitative analysis, we selected a random sample from our dataset and identified the minimum threshold that maximizes the accuracy of the Name-prism tool for each one of the race and ethnic groups. This is how we identified the different thresholds for API, Hispanic, Black, and White developers. Thus, if instead of using targeted thresholds we would have used 0.8, only 1.4% (57/4137) of our data would have been classified as "Unknown" instead of API and Hispanic developers. 

We believe that this small percentage cannot severely impact the results. But to ensure the validity of our results, we have analyzed manually the 57 developers to guess/estimate their race and ethnicity. The results of this manual analysis indicate that: (1) the number of true positives and false positives when using a threshold of 0.7 for Hispanic developers are 17/19 and 2/19, respectively; and (1) the number of true positives and false positives when using a threshold of 0.6 for API developers are 38/38 and 0/38, respectively. These results ensure that using targeted thresholds for each ethnicity does not impacts our results.
